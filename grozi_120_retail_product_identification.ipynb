{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "grozi_120_retail_product_identification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnOs_uttHjjm",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_POFHwmONnA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_formats = ['svg']\n",
        "import csv\n",
        "import datetime\n",
        "import itertools\n",
        "import json\n",
        "import os\n",
        "import tarfile\n",
        "import zipfile\n",
        "from shutil import copy, copyfile\n",
        "from textwrap import wrap\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from numpy import argmax\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import (EarlyStopping, ModelCheckpoint,\n",
        "                                        TensorBoard)\n",
        "from tensorflow.keras.metrics import (AUC, CategoricalAccuracy,\n",
        "                                      CategoricalCrossentropy, FalseNegatives,\n",
        "                                      FalsePositives,\n",
        "                                      MeanAbsolutePercentageError, Precision,\n",
        "                                      Recall, TrueNegatives, TruePositives)\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow_addons.metrics import F1Score"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBXB6Jvlh99n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7nXzCMxHr42",
        "colab_type": "text"
      },
      "source": [
        "### Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyMEnO40L1Qf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "in_situ_url = 'http://grozi.calit2.net/GroZi-120/inSitu.zip'\n",
        "in_vitro_url = 'http://grozi.calit2.net/GroZi-120/inVitro.zip'\n",
        "labels_url = 'http://grozi.calit2.net/GroZi-120/index2/UPC_index.txt'\n",
        "\n",
        "os.makedirs('temp')\n",
        "print(\"Downloading dataset.\")\n",
        "urlretrieve(in_situ_url, \"./temp/inSitu.zip\")\n",
        "urlretrieve(in_vitro_url, \"./temp/inVitro.zip\")\n",
        "urlretrieve(labels_url, \"./temp/UPC_index.txt\")\n",
        "print(\"Done.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mbx4-pKLNejP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "in_situ_path = '/content/temp/inSitu.zip'\n",
        "in_vitro_path = '/content/temp/inVitro.zip'\n",
        "labels_path = '/content/temp/UPC_index.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imxwSdPLNNsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf grozi120/\n",
        "dataset_path = '/content/grozi120/'\n",
        "new_path = '/content/grozi120/processed'\n",
        "os.makedirs('grozi120')\n",
        "with zipfile.ZipFile(in_situ_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_path)\n",
        "with zipfile.ZipFile(in_vitro_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_path)\n",
        "copyfile(labels_path, os.path.join(dataset_path, 'UPC_index.txt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p3_Sz_F2pEPG"
      },
      "source": [
        "### Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvS_pSSxCaLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resize_pad = True\n",
        "IMG_SIZE = 200\n",
        "BATCH_SIZE = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdfDtDJGnsO_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image, ImageOps\n",
        "\n",
        "def resize_with_pad(file_path, img_size):\n",
        "  img = Image.open(file_path)\n",
        "  old_size = img.size  \n",
        "  ratio = float(img_size) / max(old_size)\n",
        "  new_size = tuple([int(x * ratio) for x in old_size])\n",
        "\n",
        "  img = img.resize(new_size, Image.ANTIALIAS)\n",
        "\n",
        "  new_img = Image.new(\"RGB\", (img_size, img_size))\n",
        "  new_img.paste(img, ((img_size - new_size[0]) // 2,\n",
        "                  (img_size - new_size[1]) // 2))\n",
        "  return new_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EikF6s9hCcIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(os.path.join(new_path, 'inVitro')):\n",
        "  os.makedirs(os.path.join(new_path, 'inVitro'))\n",
        "\n",
        "labels = {}\n",
        "with open(os.path.join(dataset_path, 'UPC_index.txt'), 'r') as f:\n",
        "  lines = [line.rstrip() for line in f.readlines()[1:] if line != '\\n']\n",
        "  for i in range(0, len(lines)-2, 3):\n",
        "    labels[int(lines[i])] = [lines[i+1], lines[i+2]]\n",
        "\n",
        "dataset = {\n",
        "  'file_name': [], \n",
        "  'label': []\n",
        "}\n",
        "for product in os.listdir(os.path.join(dataset_path, 'inVitro')):\n",
        "  for product_file in os.listdir(os.path.join(dataset_path, 'inVitro', product, 'web', 'JPEG')):\n",
        "    if product_file == 'Thumbs.db':\n",
        "      continue\n",
        "    dataset['file_name'].append(os.path.join(dataset_path, 'inVitro', product, 'web', 'JPEG', product_file))\n",
        "    dataset['label'].append(labels[int(product)][1])\n",
        "\n",
        "df = pd.DataFrame(dataset, columns=['file_name', 'label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vac1S_VACdv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(os.path.join(new_path, 'inSitu')):\n",
        "    os.makedirs(os.path.join(new_path, 'inSitu'))\n",
        "\n",
        "evaluation_dataset = {\n",
        "  'file_name': [], \n",
        "  'label': []\n",
        "}\n",
        "\n",
        "for product in os.listdir(os.path.join(dataset_path, 'inSitu')):\n",
        "  for product_file in os.listdir(os.path.join(dataset_path, 'inSitu', product, 'video')):\n",
        "    if product_file == 'Thumbs.db':\n",
        "      continue\n",
        "    evaluation_dataset['file_name'].append(os.path.join(dataset_path, 'inSitu', product, 'video', product_file))\n",
        "    evaluation_dataset['label'].append(labels[int(product)][1])\n",
        "\n",
        "evaluation_df = pd.DataFrame(evaluation_dataset, columns=['file_name', 'label'])\n",
        "\n",
        "with open(os.path.join(new_path, 'inSitu', 'evaluation.csv'), mode='w') as dataset_file:\n",
        "  dataset_writer = csv.writer(dataset_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "  dataset_writer.writerow(['file_name', 'label'])\n",
        "  for file_name, label in zip(evaluation_dataset['file_name'], evaluation_dataset['label']):\n",
        "    split = file_name.split('/')\n",
        "    dataset_writer.writerow(['{}_{}'.format(split[-3], split[-1]), label])\n",
        "    if resize_pad:\n",
        "      padded_img = resize_with_pad(file_name, IMG_SIZE)\n",
        "      padded_img.save(os.path.join(new_path, 'inSitu','{}_{}'.format(split[-3], split[-1])))\n",
        "    else:\n",
        "      copyfile(file_name, os.path.join(new_path, 'inSitu','{}_{}'.format(split[-3], split[-1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouH5lNByLdej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_filenames, validation_filenames, train_labels, validation_labels = train_test_split(df['file_name'], df['label'], train_size=0.75, random_state=42, stratify=df['label'])\n",
        "\n",
        "if not os.path.exists(os.path.join(new_path, 'inVitro', 'train')):\n",
        "    os.makedirs(os.path.join(new_path, 'inVitro', 'train'))\n",
        "with open(os.path.join(new_path, 'inVitro', 'train.csv'), mode='w') as dataset_file:\n",
        "    dataset_writer = csv.writer(dataset_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    dataset_writer.writerow(['file_name', 'label'])\n",
        "    for file_name, label in zip(train_filenames, train_labels):\n",
        "        split = file_name.split('/')\n",
        "        dataset_writer.writerow(['{}_{}'.format(split[-4], split[-1]), label])\n",
        "        if resize_pad:\n",
        "          padded_img = resize_with_pad(file_name, IMG_SIZE)\n",
        "          padded_img.save(os.path.join(new_path, 'inVitro', 'train','{}_{}'.format(split[-4], split[-1])))\n",
        "        else:\n",
        "          copyfile(file_name, os.path.join(new_path, 'inVitro', 'train','{}_{}'.format(split[-4], split[-1])))\n",
        "\n",
        "if not os.path.exists(os.path.join(new_path, 'inVitro', 'validation')):\n",
        "    os.makedirs(os.path.join(new_path, 'inVitro', 'validation'))\n",
        "with open(os.path.join(new_path, 'inVitro', 'validation.csv'), mode='w') as dataset_file:\n",
        "    dataset_writer = csv.writer(dataset_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    dataset_writer.writerow(['file_name', 'label'])\n",
        "    for file_name, label in zip(validation_filenames, validation_labels):\n",
        "        split = file_name.split('/')\n",
        "        dataset_writer.writerow(['{}_{}'.format(split[-4], split[-1]), label])\n",
        "        if resize_pad:\n",
        "          padded_img = resize_with_pad(file_name, IMG_SIZE)\n",
        "          padded_img.save(os.path.join(new_path, 'inVitro', 'validation','{}_{}'.format(split[-4], split[-1])))\n",
        "        else:\n",
        "          copyfile(file_name, os.path.join(new_path, 'inVitro', 'validation','{}_{}'.format(split[-4], split[-1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaLfu3cHFm5n",
        "colab_type": "text"
      },
      "source": [
        "#### !!! Execute next cell **ONLY** if you want to train the model with both *in vitro* and *in situ* images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v79J0I2vOfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf grozi120/processed/all\n",
        "new_path = '/content/grozi120/processed/all'\n",
        "if not os.path.exists(new_path):\n",
        "    os.makedirs(new_path)\n",
        "\n",
        "insitup = '/content/grozi120/processed/inSitu'\n",
        "invitrot = '/content/grozi120/processed/inVitro/train'\n",
        "invitrov = '/content/grozi120/processed/inVitro/validation'\n",
        "folderp = [insitup, invitrot, invitrov]\n",
        "\n",
        "for p in folderp:\n",
        "  for f in os.listdir(p):\n",
        "    copyfile(os.path.join(p,f), os.path.join(new_path, f))\n",
        "\n",
        "!rm -R grozi120/processed/all/evaluation.csv\n",
        "with open(os.path.join('/content/grozi120/processed/', 'all.csv'), mode='w') as dataset_file:\n",
        "    dataset_writer = csv.writer(dataset_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    dataset_writer.writerow(['file_name', 'label'])\n",
        "    for f in os.listdir(new_path):\n",
        "      prod_num = int(f.split('_')[0])\n",
        "      dataset_writer.writerow([f, labels[prod_num][1]])\n",
        "dataset_df = pd.read_csv('/content/grozi120/processed/all.csv')\n",
        "\n",
        "train_validate_filenames, evaluation_filenames, train_validate_labels, evaluation_labels = train_test_split(dataset_df['file_name'], dataset_df['label'], train_size=0.8, random_state=42, stratify=dataset_df['label'])\n",
        "\n",
        "train_validate_df = pd.DataFrame(zip(train_validate_filenames, train_validate_labels), columns=['file_name', 'label'])\n",
        "\n",
        "train_filenames, validation_filenames, train_labels, validation_labels = train_test_split(train_validate_df['file_name'], train_validate_df['label'], train_size=0.875, random_state=42, stratify=train_validate_df['label'])\n",
        "\n",
        "train_df = pd.DataFrame(zip(train_filenames, train_labels), columns=['file_name', 'label'])\n",
        "\n",
        "validation_df = pd.DataFrame(zip(validation_filenames, validation_labels), columns=['file_name', 'label'])\n",
        "\n",
        "evaluation_df = pd.DataFrame(zip(evaluation_filenames, evaluation_labels), columns=['file_name', 'label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSdyoS3aseiT",
        "colab_type": "text"
      },
      "source": [
        "#### Define data augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWcVzZeHdrbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv(os.path.join(new_path, 'inVitro', 'train.csv'))\n",
        "validation_df = pd.read_csv(os.path.join(new_path, 'inVitro', 'validation.csv'))\n",
        "evaluation_df = pd.read_csv(os.path.join(new_path, 'inSitu', 'evaluation.csv'))\n",
        "\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    featurewise_center=False,  \n",
        "    featurewise_std_normalization=False, \n",
        "    samplewise_center=False,  \n",
        "    samplewise_std_normalization=False,  \n",
        "    zca_whitening=False,  \n",
        "    rotation_range=25,  \n",
        "    zoom_range = 0.5,\n",
        "    shear_range = 30,\n",
        "    width_shift_range=0.15,  \n",
        "    height_shift_range=0.15,\n",
        "    brightness_range=[0.25,1],\n",
        "    horizontal_flip=False,  \n",
        "    vertical_flip=False,\n",
        "    rescale=1. / 255)\n",
        "\n",
        "validation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "evaluation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    directory=os.path.join(new_path, 'inVitro', 'train'),\n",
        "    x_col=\"file_name\",\n",
        "    y_col=\"label\",\n",
        "    class_mode=\"categorical\",\n",
        "    target_size=(IMG_SIZE,IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_dataframe(\n",
        "    dataframe=validation_df,\n",
        "    directory=os.path.join(new_path, 'inVitro', 'validation'),\n",
        "    x_col=\"file_name\",\n",
        "    y_col=\"label\",\n",
        "    class_mode=\"categorical\",\n",
        "    target_size=(IMG_SIZE,IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "evaluation_generator  = evaluation_datagen.flow_from_dataframe(\n",
        "    dataframe=evaluation_df,\n",
        "    directory=os.path.join(new_path, 'inSitu'),\n",
        "    x_col=\"file_name\",\n",
        "    y_col=\"label\",\n",
        "    class_mode=\"categorical\",\n",
        "    target_size=(IMG_SIZE,IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "class_indices = evaluation_generator.class_indices\n",
        "class_indices_inverted = {v: k for k, v in class_indices.items()}\n",
        "\n",
        "x,y = train_generator.next()\n",
        "train_generator.reset()\n",
        "\n",
        "fig,ax=plt.subplots(5,5)\n",
        "fig.set_size_inches(15,15)\n",
        "num = 0\n",
        "for i in range(5):\n",
        "  for j in range (5):\n",
        "    ax[i,j].imshow(x[num])\n",
        "    mydict = evaluation_generator.class_indices\n",
        "    inverted = list(mydict.keys())[list(mydict.values()).index(argmax(y[num]))]\n",
        "    ax[i,j].set_title(\"\\n\".join(wrap(inverted, 10)))\n",
        "    ax[i,j].axis('off')\n",
        "    num += 1\n",
        "  num += 1\n",
        "        \n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VR_r5RPGtqY",
        "colab_type": "text"
      },
      "source": [
        "#### Save the mapping of the classes into a json file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD-C_JosNF_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indices = train_generator.class_indices\n",
        "with open('indices.json', 'w') as indices_json:\n",
        "    json.dump(indices, indices_json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk8927XkG5gt",
        "colab_type": "text"
      },
      "source": [
        "### Define Callbacks and Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s5ieAoXQdGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cur_date = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "log_dir = os.path.join('content', 'training', cur_date)\n",
        "model_filename = 'weights.hdf5'\n",
        "\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "callbacks = []\n",
        "early_stopping = EarlyStopping(\n",
        "  monitor='val_auc_pr',\n",
        "  verbose=1, \n",
        "  patience=10, \n",
        "  mode='max', \n",
        "  restore_best_weights=True)\n",
        "\n",
        "callbacks.append(early_stopping)\n",
        "\n",
        "tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "callbacks.append(tensorboard)\n",
        "model_checkpoint = ModelCheckpoint(model_filename, \n",
        "                                   monitor='val_auc_pr', \n",
        "                                   verbose=1, \n",
        "                                   save_best_only=True,\n",
        "                                   save_weights_only=True, \n",
        "                                   mode='max', \n",
        "                                   save_freq='epoch', \n",
        ")\n",
        "callbacks.append(model_checkpoint)\n",
        "\n",
        "num_classes = len(train_generator.class_indices)\n",
        "\n",
        "metrics = [CategoricalAccuracy(name='categorical_accuracy'),\n",
        "    Precision(name='precision'),\n",
        "    Recall(name='recall'),\n",
        "    AUC(name='auc_pr', curve='PR'),\n",
        "    AUC(name='auc_roc', curve='ROC'),\n",
        "    F1Score(name='f1score', num_classes=num_classes), \n",
        "    TrueNegatives(name='tn'), \n",
        "    TruePositives(name='tp'), \n",
        "    FalseNegatives(name='fn'), \n",
        "    FalsePositives(name='fp')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJcka1pVHCNb",
        "colab_type": "text"
      },
      "source": [
        "### Define model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjLO5lT0H-AL",
        "colab_type": "text"
      },
      "source": [
        "#### Create base model with ImageNet weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7S677bOQf8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
        "\n",
        "base_model = tf.keras.applications.InceptionResNetV2(\n",
        "    input_shape=IMG_SHAPE,\n",
        "    include_top=False,\n",
        "    weights='imagenet')\n",
        "                                    \n",
        "base_model.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hItNVBzYIEGa",
        "colab_type": "text"
      },
      "source": [
        "#### Define layers for new classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCnPq1GrNtWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "regularizer = tf.keras.regularizers.L1L2(l1=0.0001, l2=0.0001)\n",
        "global_average_pooling_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "dropout_layer = tf.keras.layers.Dropout(0.3)\n",
        "dense = tf.keras.layers.Dense(2048, activation='relu')\n",
        "prediction_layer = tf.keras.layers.Dense(num_classes, activation='softmax', activity_regularizer=regularizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VWqFNCsIKxC",
        "colab_type": "text"
      },
      "source": [
        "#### Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZQRcB4tQl9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "x = base_model(inputs, training=False)\n",
        "x = global_average_pooling_layer(x)\n",
        "x = dropout_layer(x)\n",
        "x = dense(x)\n",
        "y = prediction_layer(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpTrLFb-SBbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_learning_rate = 0.00005\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=metrics)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucOr-3FVHMSs",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUOrVoqRIpFn",
        "colab_type": "text"
      },
      "source": [
        "#### Initial training with frozen base model layers to fit the new classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_6PhqtVQqvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 300\n",
        "\n",
        "history = model.fit(train_generator, epochs=num_epochs, validation_data=validation_generator,  callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MDKY_Q1I1Cd",
        "colab_type": "text"
      },
      "source": [
        "#### Save training hisory to a json file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U384JDq-I5it",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "hist_json_file = '/content/history.json'\n",
        "with open(hist_json_file, mode='w') as f:\n",
        "    hist_df.to_json(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBQrSaXwQPp-",
        "colab_type": "text"
      },
      "source": [
        "### Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve6EzVQ1IYlt",
        "colab_type": "text"
      },
      "source": [
        "#### Definition of callbacks for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6gghwLm5qpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cur_date = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "log_dir = os.path.join('content', 'training', cur_date)\n",
        "model_filename = 'fine_tuning_weights.hdf5'\n",
        "checkpoint_path = os.path.join(log_dir, 'checkpoints', model_filename)\n",
        "\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "callbacks_ft = []\n",
        "early_stopping_ft = EarlyStopping(\n",
        "  monitor='val_auc_pr',\n",
        "  verbose=1, \n",
        "  patience=10, \n",
        "  mode='max', \n",
        "  restore_best_weights=True)\n",
        "\n",
        "callbacks_ft.append(early_stopping_ft)\n",
        "\n",
        "tensorboard_ft = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "callbacks_ft.append(tensorboard_ft)\n",
        "model_checkpoint_ft = ModelCheckpoint(model_filename, \n",
        "                                   monitor='val_auc_pr', \n",
        "                                   verbose=1, \n",
        "                                   save_best_only=True,\n",
        "                                   save_weights_only=True, \n",
        "                                   mode='max', \n",
        "                                   save_freq='epoch', \n",
        ")\n",
        "callbacks_ft.append(model_checkpoint_ft)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEpLLxbKIfHu",
        "colab_type": "text"
      },
      "source": [
        "#### Recompile model with unfrozen base model layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyTB7VLsP_cN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_model.trainable = True\n",
        "\n",
        "base_learning_rate = 0.000001\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=metrics)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "276EEEcCIqz4",
        "colab_type": "text"
      },
      "source": [
        "#### Fine-tune the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF2L6eJUYbtJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fine_tune_epochs = 20\n",
        "\n",
        "total_epochs =  history.epoch[-1] + 1 + fine_tune_epochs\n",
        "\n",
        "history_fine = model.fit(train_generator,\n",
        "                         epochs=total_epochs,\n",
        "                         initial_epoch=history.epoch[-1],\n",
        "                         validation_data=validation_generator, callbacks=callbacks_ft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMbBCqYPJABo",
        "colab_type": "text"
      },
      "source": [
        "#### Save fine-tuning history to a json file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0pa0O-5ANyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist_ft_df = pd.DataFrame(history_fine.history) \n",
        "\n",
        "hist_ft_json_file = '/content/history_fine.json'\n",
        "with open(hist_ft_json_file, mode='w') as f:\n",
        "    hist_ft_df.to_json(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGVT2tpCJig8",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j35WTkStBxIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluation = model.evaluate(evaluation_generator)\n",
        "\n",
        "evaluation_data_df = pd.DataFrame(evaluation) \n",
        "\n",
        "evaluation_json_file = '/content/evaluation.json'\n",
        "with open(evaluation_json_file, mode='w') as f:\n",
        "    evaluation_data_df.to_json(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47JS5Z-7Jao7",
        "colab_type": "text"
      },
      "source": [
        "### Save and load model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0umpzF9UyfPW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path =  '/content/drive/My Drive/Colab Notebooks/models/grozi120/InceptionResNetV2_imagenet_{}.h5'.format(datetime.datetime.now().strftime(\"%d_%m_%y_%H_%M_%S\"))\n",
        "model.save(model_path) \n",
        "model_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIyJhru8Px5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sabiALXoJ5hx",
        "colab_type": "text"
      },
      "source": [
        "### Plot confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acqPvCGEpzOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from https://deeplizard.com/learn/video/km7pxKy4UHU\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues, file_name='confusion_matrix'):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure(figsize=(64,64))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.savefig(os.path.join('content', file_name) + '.svg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp7nTcqMpwda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_image (path):\n",
        "    img = image.load_img(path, target_size=(IMG_SIZE, IMG_SIZE))\n",
        "    x = np.array(img).astype('float64')/255\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEg7IdnyBuD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_numbers_predicted = []\n",
        "class_numbers_actual = []\n",
        "\n",
        "labels_ = evaluation_df.groupby(\"label\")\n",
        "for label in labels_:\n",
        "  for path in label[1][\"file_name\"]:\n",
        "    x = get_image(os.path.join('/content/grozi120/processed/all', path))\n",
        "    p = model.predict(x)\n",
        "    predicted = class_indices_inverted[argmax(p)]\n",
        "    class_numbers_predicted.append(argmax(p))\n",
        "    actual = label[0]\n",
        "    class_numbers_actual.append(class_indices[actual])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuIsSs3sIRwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm = confusion_matrix(y_true=class_numbers_actual, y_pred=class_numbers_predicted)\n",
        "cm_plot_labels = evaluation_generator.class_indices\n",
        "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, normalize=False, title='Confusion Matrix GroZi-120', file_name='confusion_matrix_grozi120')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}